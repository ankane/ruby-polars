module Polars
  module IO
    # Read a CSV file into a DataFrame.
    #
    # @param source [Object]
    #   Path to a file or a file-like object.
    # @param has_header [Boolean]
    #   Indicate if the first row of dataset is a header or not.
    #   If set to false, column names will be autogenerated in the
    #   following format: `column_x`, with `x` being an
    #   enumeration over every column in the dataset starting at 1.
    # @param columns [Object]
    #   Columns to select. Accepts a list of column indices (starting
    #   at zero) or a list of column names.
    # @param new_columns [Object]
    #   Rename columns right after parsing the CSV file. If the given
    #   list is shorter than the width of the DataFrame the remaining
    #   columns will have their original name.
    # @param separator [String]
    #   Single byte character to use as separator in the file.
    # @param comment_prefix [String]
    #   A string used to indicate the start of a comment line. Comment lines are skipped
    #   during parsing. Common examples of comment prefixes are `#` and `//`.
    # @param quote_char [String]
    #   Single byte character used for csv quoting.
    #   Set to nil to turn off special handling and escaping of quotes.
    # @param skip_rows [Integer]
    #   Start reading after `skip_rows` lines.
    # @param skip_lines [Integer]
    #   Start reading after `skip_lines` lines. The header will be parsed at this
    #   offset. Note that CSV escaping will not be respected when skipping lines.
    #   If you want to skip valid CSV rows, use `skip_rows`.
    # @param schema [Object]
    #   Provide the schema. This means that polars doesn't do schema inference.
    #   This argument expects the complete schema, whereas `schema_overrides` can be
    #   used to partially overwrite a schema. Note that the order of the columns in
    #   the provided `schema` must match the order of the columns in the CSV being read.
    # @param schema_overrides [Object]
    #   Overwrite dtypes for specific or all columns during schema inference.
    # @param null_values [Object]
    #   Values to interpret as null values. You can provide a:
    #
    #   - `String`: All values equal to this string will be null.
    #   - `Array`: All values equal to any string in this array will be null.
    #   - `Hash`: A hash that maps column name to a null value string.
    # @param missing_utf8_is_empty_string [Boolean]
    #   By default a missing value is considered to be null; if you would prefer missing
    #   utf8 values to be treated as the empty string you can set this param true.
    # @param ignore_errors [Boolean]
    #   Try to keep reading lines if some lines yield errors.
    #   First try `infer_schema_length: 0` to read all columns as
    #   `:str` to check which values might cause an issue.
    # @param try_parse_dates [Boolean]
    #   Try to automatically parse dates. If this does not succeed,
    #   the column remains of data type `:str`.
    # @param n_threads [Integer]
    #   Number of threads to use in csv parsing.
    #   Defaults to the number of physical cpu's of your system.
    # @param infer_schema [Boolean]
    #   When `true`, the schema is inferred from the data using the first
    #   `infer_schema_length` rows.
    #   When `false`, the schema is not inferred and will be `Polars::String` if not
    #   specified in `schema` or `schema_overrides`.
    # @param infer_schema_length [Integer]
    #   The maximum number of rows to scan for schema inference.
    #   If set to `nil`, the full data may be scanned *(this is slow)*.
    #   Set `infer_schema: false` to read all columns as `Polars::String`.
    # @param batch_size [Integer]
    #   Number of lines to read into the buffer at once.
    #   Modify this to change performance.
    # @param n_rows [Integer]
    #   Stop reading from CSV file after reading `n_rows`.
    #   During multi-threaded parsing, an upper bound of `n_rows`
    #   rows cannot be guaranteed.
    # @param encoding ["utf8", "utf8-lossy"]
    #   Lossy means that invalid utf8 values are replaced with `�`
    #   characters. When using other encodings than `utf8` or
    #   `utf8-lossy`, the input is first decoded im memory with
    #   Ruby.
    # @param low_memory [Boolean]
    #   Reduce memory usage at expense of performance.
    # @param rechunk [Boolean]
    #   Make sure that all columns are contiguous in memory by
    #   aggregating the chunks into a single array.
    # @param storage_options [Hash]
    #   Extra options that make sense for a
    #   particular storage connection.
    # @param skip_rows_after_header [Integer]
    #   Skip this number of rows when the header is parsed.
    # @param row_index_name [String]
    #   If not nil, this will insert a row count column with the given name into
    #   the DataFrame.
    # @param row_index_offset [Integer]
    #   Offset to start the row_count column (only used if the name is set).
    # @param eol_char [String]
    #   Single byte end of line character.
    # @param raise_if_empty [Boolean]
    #   When there is no data in the source, `NoDataError` is raised. If this parameter
    #   is set to false, an empty DataFrame (with no columns) is returned instead.
    # @param truncate_ragged_lines [Boolean]
    #   Truncate lines that are longer than the schema.
    # @param decimal_comma [Boolean]
    #   Parse floats using a comma as the decimal separator instead of a period.
    # @param glob [Boolean]
    #   Expand path given via globbing rules.
    #
    # @return [DataFrame]
    #
    # @note
    #   This operation defaults to a `rechunk` operation at the end, meaning that
    #   all data will be stored continuously in memory.
    #   Set `rechunk: false` if you are benchmarking the csv-reader. A `rechunk` is
    #   an expensive operation.
    def read_csv(
      source,
      has_header: true,
      columns: nil,
      new_columns: nil,
      separator: ",",
      comment_prefix: nil,
      quote_char: '"',
      skip_rows: 0,
      skip_lines: 0,
      schema: nil,
      schema_overrides: nil,
      null_values: nil,
      missing_utf8_is_empty_string: false,
      ignore_errors: false,
      try_parse_dates: false,
      n_threads: nil,
      infer_schema: true,
      infer_schema_length: N_INFER_DEFAULT,
      batch_size: 8192,
      n_rows: nil,
      encoding: "utf8",
      low_memory: false,
      rechunk: true,
      storage_options: nil,
      skip_rows_after_header: 0,
      row_index_name: nil,
      row_index_offset: 0,
      eol_char: "\n",
      raise_if_empty: false,
      truncate_ragged_lines: false,
      decimal_comma: false,
      glob: true
    )
      Utils._check_arg_is_1byte("separator", separator, false)
      Utils._check_arg_is_1byte("quote_char", quote_char, true)
      Utils._check_arg_is_1byte("eol_char", eol_char, false)

      projection, columns = Utils.handle_projection_columns(columns)

      storage_options ||= {}

      if columns && !has_header
        columns.each do |column|
          if !column.start_with?("column_")
            raise ArgumentError, "Specified column names do not start with \"column_\", but autogenerated header names were requested."
          end
        end
      end

      if !infer_schema
        infer_schema_length = 0
      end

      df = nil
      _prepare_file_arg(source) do |data|
        df = _read_csv_impl(
          data,
          has_header: has_header,
          columns: columns || projection,
          separator: separator,
          comment_prefix: comment_prefix,
          quote_char: quote_char,
          skip_rows: skip_rows,
          skip_lines: skip_lines,
          schema_overrides: schema_overrides,
          schema: schema,
          null_values: null_values,
          missing_utf8_is_empty_string: missing_utf8_is_empty_string,
          ignore_errors: ignore_errors,
          try_parse_dates: try_parse_dates,
          n_threads: n_threads,
          infer_schema_length: infer_schema_length,
          batch_size: batch_size,
          n_rows: n_rows,
          encoding: encoding == "utf8-lossy" ? encoding : "utf8",
          low_memory: low_memory,
          rechunk: rechunk,
          skip_rows_after_header: skip_rows_after_header,
          row_index_name: row_index_name,
          row_index_offset: row_index_offset,
          eol_char: eol_char,
          raise_if_empty: raise_if_empty,
          truncate_ragged_lines: truncate_ragged_lines,
          decimal_comma: decimal_comma,
          glob: glob
        )
      end

      if new_columns
        Utils._update_columns(df, new_columns)
      else
        df
      end
    end

    # @private
    def _read_csv_impl(
      file,
      has_header: true,
      columns: nil,
      separator: ",",
      comment_prefix: nil,
      quote_char: '"',
      skip_rows: 0,
      skip_lines: 0,
      schema: nil,
      schema_overrides: nil,
      null_values: nil,
      missing_utf8_is_empty_string: false,
      ignore_errors: false,
      try_parse_dates: false,
      n_threads: nil,
      infer_schema_length: N_INFER_DEFAULT,
      batch_size: 8192,
      n_rows: nil,
      encoding: "utf8",
      low_memory: false,
      rechunk: false,
      skip_rows_after_header: 0,
      row_index_name: nil,
      row_index_offset: 0,
      eol_char: "\n",
      raise_if_empty: true,
      truncate_ragged_lines: false,
      decimal_comma: false,
      glob: true
    )
      if Utils.pathlike?(file)
        path = Utils.normalize_filepath(file)
      else
        path = nil
        # if defined?(StringIO) && file.is_a?(StringIO)
        #   file = file.string
        # end
      end

      dtype_list = nil
      dtype_slice = nil
      if !schema_overrides.nil?
        if schema_overrides.is_a?(Hash)
          dtype_list = []
          schema_overrides.each do |k, v|
            dtype_list << [k, Utils.rb_type_to_dtype(v)]
          end
        elsif schema_overrides.is_a?(::Array)
          dtype_slice = schema_overrides
        else
          # TODO improve type and message
          raise ArgumentError, "dtype arg should be list or dict"
        end
      end

      processed_null_values = Utils._process_null_values(null_values)

      if columns.is_a?(::String)
        columns = [columns]
      end
      if file.is_a?(::String) && file.include?("*")
        dtypes_dict = nil
        if !dtype_list.nil?
          dtypes_dict = dtype_list.to_h
        end
        if !dtype_slice.nil?
          raise ArgumentError, "cannot use glob patterns and unnamed dtypes as `dtypes` argument; Use dtypes: Mapping[str, Type[DataType]"
        end
        scan = scan_csv(
          file,
          has_header: has_header,
          separator: separator,
          comment_prefix: comment_prefix,
          quote_char: quote_char,
          skip_rows: skip_rows,
          skip_lines: skip_lines,
          schema: schema,
          schema_overrides: dtypes_dict,
          null_values: null_values,
          missing_utf8_is_empty_string: missing_utf8_is_empty_string,
          ignore_errors: ignore_errors,
          infer_schema_length: infer_schema_length,
          n_rows: n_rows,
          low_memory: low_memory,
          rechunk: rechunk,
          skip_rows_after_header: skip_rows_after_header,
          row_index_name: row_index_name,
          row_index_offset: row_index_offset,
          eol_char: eol_char,
          raise_if_empty: raise_if_empty,
          truncate_ragged_lines: truncate_ragged_lines,
          decimal_comma: decimal_comma,
          glob: glob
        )
        if columns.nil?
          return scan.collect
        elsif is_str_sequence(columns, allow_str: false)
          return scan.select(columns).collect
        else
          raise ArgumentError, "cannot use glob patterns and integer based projection as `columns` argument; Use columns: List[str]"
        end
      end

      projection, columns = Utils.handle_projection_columns(columns)

      rbdf =
        RbDataFrame.read_csv(
          file,
          infer_schema_length,
          batch_size,
          has_header,
          ignore_errors,
          n_rows,
          skip_rows,
          skip_lines,
          projection,
          separator,
          rechunk,
          columns,
          encoding,
          n_threads,
          path,
          dtype_list,
          dtype_slice,
          low_memory,
          comment_prefix,
          quote_char,
          processed_null_values,
          missing_utf8_is_empty_string,
          try_parse_dates,
          skip_rows_after_header,
          Utils.parse_row_index_args(row_index_name, row_index_offset),
          eol_char,
          raise_if_empty,
          truncate_ragged_lines,
          decimal_comma,
          schema
        )
      Utils.wrap_df(rbdf)
    end

    # Read a CSV file in batches.
    #
    # Upon creation of the `BatchedCsvReader`,
    # polars will gather statistics and determine the
    # file chunks. After that work will only be done
    # if `next_batches` is called.
    #
    # @param source [Object]
    #   Path to a file or a file-like object.
    # @param has_header [Boolean]
    #   Indicate if the first row of dataset is a header or not.
    #   If set to False, column names will be autogenerated in the
    #   following format: `column_x`, with `x` being an
    #   enumeration over every column in the dataset starting at 1.
    # @param columns [Object]
    #   Columns to select. Accepts a list of column indices (starting
    #   at zero) or a list of column names.
    # @param new_columns [Object]
    #   Rename columns right after parsing the CSV file. If the given
    #   list is shorter than the width of the DataFrame the remaining
    #   columns will have their original name.
    # @param separator [String]
    #   Single byte character to use as separator in the file.
    # @param comment_prefix [String]
    #   A string used to indicate the start of a comment line. Comment lines are skipped
    #   during parsing. Common examples of comment prefixes are `#` and `//`.
    # @param quote_char [String]
    #   Single byte character used for csv quoting, default = `"`.
    #   Set to nil to turn off special handling and escaping of quotes.
    # @param skip_rows [Integer]
    #   Start reading after `skip_rows` lines.
    # @param skip_lines [Integer]
    #   Start reading after `skip_lines` lines. The header will be parsed at this
    #   offset. Note that CSV escaping will not be respected when skipping lines.
    #   If you want to skip valid CSV rows, use `skip_rows`.
    # @param schema_overrides [Object]
    #   Overwrite dtypes during inference.
    # @param null_values [Object]
    #   Values to interpret as null values. You can provide a:
    #
    #   - `String`: All values equal to this string will be null.
    #   - `Array`: All values equal to any string in this array will be null.
    #   - `Hash`: A hash that maps column name to a null value string.
    # @param missing_utf8_is_empty_string [Boolean]
    #   By default a missing value is considered to be null; if you would prefer missing
    #   utf8 values to be treated as the empty string you can set this param true.
    # @param ignore_errors [Boolean]
    #   Try to keep reading lines if some lines yield errors.
    #   First try `infer_schema_length: 0` to read all columns as
    #   `:str` to check which values might cause an issue.
    # @param try_parse_dates [Boolean]
    #   Try to automatically parse dates. If this does not succeed,
    #   the column remains of data type `:str`.
    # @param n_threads [Integer]
    #   Number of threads to use in csv parsing.
    #   Defaults to the number of physical cpu's of your system.
    # @param infer_schema_length [Integer]
    #   Maximum number of lines to read to infer schema.
    #   If set to 0, all columns will be read as `:str`.
    #   If set to `nil`, a full table scan will be done (slow).
    # @param batch_size [Integer]
    #   Number of lines to read into the buffer at once.
    #   Modify this to change performance.
    # @param n_rows [Integer]
    #   Stop reading from CSV file after reading `n_rows`.
    #   During multi-threaded parsing, an upper bound of `n_rows`
    #   rows cannot be guaranteed.
    # @param encoding ["utf8", "utf8-lossy"]
    #   Lossy means that invalid utf8 values are replaced with `�`
    #   characters. When using other encodings than `utf8` or
    #   `utf8-lossy`, the input is first decoded im memory with
    #   Ruby. Defaults to `utf8`.
    # @param low_memory [Boolean]
    #   Reduce memory usage at expense of performance.
    # @param rechunk [Boolean]
    #   Make sure that all columns are contiguous in memory by
    #   aggregating the chunks into a single array.
    # @param skip_rows_after_header [Integer]
    #   Skip this number of rows when the header is parsed.
    # @param row_index_name [String]
    #   If not nil, this will insert a row count column with the given name into
    #   the DataFrame.
    # @param row_index_offset [Integer]
    #   Offset to start the row_count column (only used if the name is set).
    # @param eol_char [String]
    #   Single byte end of line character.
    # @param raise_if_empty [Boolean]
    #   When there is no data in the source,`NoDataError` is raised. If this parameter
    #   is set to false, `nil` will be returned from `next_batches(n)` instead.
    # @param truncate_ragged_lines [Boolean]
    #   Truncate lines that are longer than the schema.
    # @param decimal_comma [Boolean]
    #   Parse floats using a comma as the decimal separator instead of a period.
    #
    # @return [BatchedCsvReader]
    #
    # @example
    #   reader = Polars.read_csv_batched(
    #     "./tpch/tables_scale_100/lineitem.tbl", separator: "|", try_parse_dates: true
    #   )
    #   reader.next_batches(5)
    def read_csv_batched(
      source,
      has_header: true,
      columns: nil,
      new_columns: nil,
      separator: ",",
      comment_prefix: nil,
      quote_char: '"',
      skip_rows: 0,
      skip_lines: 0,
      schema_overrides: nil,
      null_values: nil,
      missing_utf8_is_empty_string: false,
      ignore_errors: false,
      try_parse_dates: nil,
      n_threads: nil,
      infer_schema_length: N_INFER_DEFAULT,
      batch_size: 50_000,
      n_rows: nil,
      encoding: "utf8",
      low_memory: false,
      rechunk: true,
      skip_rows_after_header: 0,
      row_index_name: nil,
      row_index_offset: 0,
      eol_char: "\n",
      raise_if_empty: true,
      truncate_ragged_lines: false,
      decimal_comma: false
    )
      projection, columns = Utils.handle_projection_columns(columns)

      if columns && !has_header
        columns.each do |column|
          if !column.start_with?("column_")
            raise ArgumentError, "Specified column names do not start with \"column_\", but autogenerated header names were requested."
          end
        end
      end

      BatchedCsvReader.new(
        source,
        has_header: has_header,
        columns: columns || projection,
        separator: separator,
        comment_prefix: comment_prefix,
        quote_char: quote_char,
        skip_rows: skip_rows,
        skip_lines: skip_lines,
        schema_overrides: schema_overrides,
        null_values: null_values,
        missing_utf8_is_empty_string: missing_utf8_is_empty_string,
        ignore_errors: ignore_errors,
        try_parse_dates: try_parse_dates,
        n_threads: n_threads,
        infer_schema_length: infer_schema_length,
        batch_size: batch_size,
        n_rows: n_rows,
        encoding: encoding == "utf8-lossy" ? encoding : "utf8",
        low_memory: low_memory,
        rechunk: rechunk,
        skip_rows_after_header: skip_rows_after_header,
        row_index_name: row_index_name,
        row_index_offset: row_index_offset,
        eol_char: eol_char,
        new_columns: new_columns,
        raise_if_empty: raise_if_empty,
        truncate_ragged_lines: truncate_ragged_lines,
        decimal_comma: decimal_comma
      )
    end

    # Lazily read from a CSV file or multiple files via glob patterns.
    #
    # This allows the query optimizer to push down predicates and
    # projections to the scan level, thereby potentially reducing
    # memory overhead.
    #
    # @param source [Object]
    #   Path to a file.
    # @param has_header [Boolean]
    #   Indicate if the first row of dataset is a header or not.
    #   If set to false, column names will be autogenerated in the
    #   following format: `column_x`, with `x` being an
    #   enumeration over every column in the dataset starting at 1.
    # @param separator [String]
    #   Single byte character to use as separator in the file.
    # @param comment_prefix [String]
    #   A string used to indicate the start of a comment line. Comment lines are skipped
    #   during parsing. Common examples of comment prefixes are `#` and `//`.
    # @param quote_char [String]
    #   Single byte character used for csv quoting.
    #   Set to nil to turn off special handling and escaping of quotes.
    # @param skip_rows [Integer]
    #   Start reading after `skip_rows` lines. The header will be parsed at this
    #   offset.
    # @param skip_lines [Integer]
    #   Start reading after `skip_lines` lines. The header will be parsed at this
    #   offset. Note that CSV escaping will not be respected when skipping lines.
    #   If you want to skip valid CSV rows, use `skip_rows`.
    # @param schema [Object]
    #   Provide the schema. This means that polars doesn't do schema inference.
    #   This argument expects the complete schema, whereas `schema_overrides` can be
    #   used to partially overwrite a schema. Note that the order of the columns in
    #   the provided `schema` must match the order of the columns in the CSV being read.
    # @param schema_overrides [Object]
    #   Overwrite dtypes for specific or all columns during schema inference.
    # @param null_values [Object]
    #   Values to interpret as null values. You can provide a:
    #
    #   - `String`: All values equal to this string will be null.
    #   - `Array`: All values equal to any string in this array will be null.
    #   - `Hash`: A hash that maps column name to a null value string.
    # @param missing_utf8_is_empty_string [Boolean]
    #   By default a missing value is considered to be null; if you would prefer missing
    #   utf8 values to be treated as the empty string you can set this param true.
    # @param ignore_errors [Boolean]
    #   Try to keep reading lines if some lines yield errors.
    #   First try `infer_schema_length: 0` to read all columns as
    #   `:str` to check which values might cause an issue.
    # @param cache [Boolean]
    #   Cache the result after reading.
    # @param with_column_names [Object]
    #   Apply a function over the column names.
    #   This can be used to update a schema just in time, thus before
    #   scanning.
    # @param infer_schema [Boolean]
    #   When `true`, the schema is inferred from the data using the first
    #   `infer_schema_length` rows.
    #   When `false`, the schema is not inferred and will be `Polars::String` if not
    #   specified in `schema` or `schema_overrides`.
    # @param infer_schema_length [Integer]
    #   Maximum number of lines to read to infer schema.
    #   If set to 0, all columns will be read as `:str`.
    #   If set to `nil`, a full table scan will be done (slow).
    # @param n_rows [Integer]
    #   Stop reading from CSV file after reading `n_rows`.
    # @param encoding ["utf8", "utf8-lossy"]
    #   Lossy means that invalid utf8 values are replaced with `�`
    #   characters.
    # @param low_memory [Boolean]
    #   Reduce memory usage in expense of performance.
    # @param rechunk [Boolean]
    #   Reallocate to contiguous memory when all chunks/ files are parsed.
    # @param skip_rows_after_header [Integer]
    #   Skip this number of rows when the header is parsed.
    # @param row_index_name [String]
    #   If not nil, this will insert a row count column with the given name into
    #   the DataFrame.
    # @param row_index_offset [Integer]
    #   Offset to start the row_count column (only used if the name is set).
    # @param try_parse_dates [Boolean]
    #   Try to automatically parse dates. If this does not succeed,
    #   the column remains of data type `:str`.
    # @param eol_char [String]
    #   Single byte end of line character.
    # @param new_columns [Array]
    #   Provide an explicit list of string column names to use (for example, when
    #   scanning a headerless CSV file). If the given list is shorter than the width of
    #   the DataFrame the remaining columns will have their original name.
    # @param raise_if_empty [Boolean]
    #   When there is no data in the source, `NoDataError` is raised. If this parameter
    #   is set to false, an empty LazyFrame (with no columns) is returned instead.
    # @param truncate_ragged_lines [Boolean]
    #   Truncate lines that are longer than the schema.
    # @param decimal_comma [Boolean]
    #   Parse floats using a comma as the decimal separator instead of a period.
    # @param glob [Boolean]
    #   Expand path given via globbing rules.
    # @param storage_options [Hash]
    #   Options that indicate how to connect to a cloud provider.
    #
    #   The cloud providers currently supported are AWS, GCP, and Azure.
    #   See supported keys here:
    #
    #   * [aws](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
    #   * [gcp](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
    #   * [azure](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
    #   * Hugging Face (`hf://`): Accepts an API key under the `token` parameter: \
    #     `{'token': '...'}`, or by setting the `HF_TOKEN` environment variable.
    #
    #   If `storage_options` is not provided, Polars will try to infer the information
    #   from environment variables.
    # @param credential_provider [Object]
    #   Provide a function that can be called to provide cloud storage
    #   credentials. The function is expected to return a dictionary of
    #   credential keys along with an optional credential expiry time.
    # @param retries [Integer]
    #   Number of retries if accessing a cloud instance fails.
    # @param file_cache_ttl [Integer]
    #   Amount of time to keep downloaded cloud files since their last access time,
    #   in seconds. Uses the `POLARS_FILE_CACHE_TTL` environment variable
    #   (which defaults to 1 hour) if not given.
    # @param include_file_paths [String]
    #   Include the path of the source file(s) as a column with this name.
    #
    # @return [LazyFrame]
    def scan_csv(
      source,
      has_header: true,
      separator: ",",
      comment_prefix: nil,
      quote_char: '"',
      skip_rows: 0,
      skip_lines: 0,
      schema: nil,
      schema_overrides: nil,
      null_values: nil,
      missing_utf8_is_empty_string: false,
      ignore_errors: false,
      cache: true,
      with_column_names: nil,
      infer_schema: true,
      infer_schema_length: N_INFER_DEFAULT,
      n_rows: nil,
      encoding: "utf8",
      low_memory: false,
      rechunk: false,
      skip_rows_after_header: 0,
      row_index_name: nil,
      row_index_offset: 0,
      try_parse_dates: false,
      eol_char: "\n",
      new_columns: nil,
      raise_if_empty: true,
      truncate_ragged_lines: false,
      decimal_comma: false,
      glob: true,
      storage_options: nil,
      credential_provider: "auto",
      retries: 2,
      file_cache_ttl: nil,
      include_file_paths: nil
    )
      if new_columns
        raise Todo
      end

      Utils._check_arg_is_1byte("separator", separator, false)
      Utils._check_arg_is_1byte("quote_char", quote_char, true)

      if Utils.pathlike?(source)
        source = Utils.normalize_filepath(source)
      end

      if !infer_schema
        infer_schema_length = 0
      end

      credential_provider_builder = _init_credential_provider_builder(
        credential_provider, source, storage_options, "scan_csv"
      )

      _scan_csv_impl(
        source,
        has_header: has_header,
        separator: separator,
        comment_prefix: comment_prefix,
        quote_char: quote_char,
        skip_rows: skip_rows,
        skip_lines: skip_lines,
        schema_overrides: schema_overrides,
        schema: schema,
        null_values: null_values,
        ignore_errors: ignore_errors,
        cache: cache,
        with_column_names: with_column_names,
        infer_schema_length: infer_schema_length,
        n_rows: n_rows,
        low_memory: low_memory,
        rechunk: rechunk,
        skip_rows_after_header: skip_rows_after_header,
        encoding: encoding,
        row_index_name: row_index_name,
        row_index_offset: row_index_offset,
        try_parse_dates: try_parse_dates,
        eol_char: eol_char,
        raise_if_empty: raise_if_empty,
        truncate_ragged_lines: truncate_ragged_lines,
        decimal_comma: decimal_comma,
        glob: glob,
        retries: retries,
        storage_options: storage_options,
        credential_provider: credential_provider_builder,
        file_cache_ttl: file_cache_ttl,
        include_file_paths: include_file_paths
      )
    end

    # @private
    def _scan_csv_impl(
      source,
      has_header: true,
      separator: ",",
      comment_prefix: nil,
      quote_char: '"',
      skip_rows: 0,
      skip_lines: 0,
      schema: nil,
      schema_overrides: nil,
      null_values: nil,
      missing_utf8_is_empty_string: false,
      ignore_errors: false,
      cache: true,
      with_column_names: nil,
      infer_schema_length: N_INFER_DEFAULT,
      n_rows: nil,
      encoding: "utf8",
      low_memory: false,
      rechunk: false,
      skip_rows_after_header: 0,
      row_index_name: nil,
      row_index_offset: 0,
      try_parse_dates: false,
      eol_char: "\n",
      raise_if_empty: true,
      truncate_ragged_lines: true,
      decimal_comma: false,
      glob: true,
      storage_options: nil,
      credential_provider: nil,
      retries: 2,
      file_cache_ttl: nil,
      include_file_paths: nil
    )
      dtype_list = nil
      if !schema_overrides.nil?
        dtype_list = []
        schema_overrides.each do |k, v|
          dtype_list << [k, Utils.rb_type_to_dtype(v)]
        end
      end
      processed_null_values = Utils._process_null_values(null_values)

      if source.is_a?(::Array)
        sources = source
        source = nil
      else
        sources = []
      end

      rblf =
        RbLazyFrame.new_from_csv(
          source,
          sources,
          separator,
          has_header,
          ignore_errors,
          skip_rows,
          skip_lines,
          n_rows,
          cache,
          dtype_list,
          low_memory,
          comment_prefix,
          quote_char,
          processed_null_values,
          missing_utf8_is_empty_string,
          infer_schema_length,
          with_column_names,
          rechunk,
          skip_rows_after_header,
          encoding,
          Utils.parse_row_index_args(row_index_name, row_index_offset),
          try_parse_dates,
          eol_char,
          raise_if_empty,
          truncate_ragged_lines,
          decimal_comma,
          glob,
          schema,
          storage_options,
          credential_provider,
          retries,
          file_cache_ttl,
          include_file_paths
        )
      Utils.wrap_ldf(rblf)
    end

    private

    def _prepare_file_arg(file)
      if file.is_a?(::String) && file =~ /\Ahttps?:\/\//
        require "uri"

        file = URI(file)
      end

      if defined?(URI) && file.is_a?(URI)
        require "open-uri"

        file = file.open
      end

      yield file
    end
  end
end
